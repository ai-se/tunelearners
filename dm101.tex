
Human programmers are clever, but flawed. Coding  adds functionality, but also defects.
Hence, software sometimes crashes (perhaps at the most awkward or dangerous moment) or delivers
the wrong functionality. For a very long list of software-related errors,
see  Peter Neumann's ``Risk Digest'' at catless.ncl.ac.uk/Risks.

Since programming inherently
introduces defects into  programs, it is important to test them before they're used.
Testing is expensive.
Software assessment budgets are finite
while assessment effectiveness increases 
exponentially with assessment effort.
For example, for  black-box testing methods,
a {\em linear} increase
in the confidence $C$ of finding  defects
can take {\em exponentially} more effort\footnote{A randomly selected 
input to a program will find a fault with probability $p$.
After $N$ random black-box tests, the chances of the inputs 
not revealing any fault 
is $(1-p)^N$. Hence, the chances $C$ of seeing the fault is $1-(1-p)^N$
which can be rearranged to 
 $N(C,p)=log(1 -
C)/log(1-p)$. For example, $N(0.90,10^{-3})=2301$
but $N(0.98,10^{-3})=3901$; i.e. nearly double the number of tests.}.
Exponential costs quickly exhaust finite resources so
standard practice is to apply the best
available  methods on code sections that seem   most critical. 
But 
any method that focuses on parts of the code
can blind us to defects in other areas. Some  {\em
lightweight sampling policy} should be used to explore the rest of the system.  This
sampling policy will always be incomplete.
Nevertheless, it is the only option when
resources prevent a complete assessment of everything.

One such lightweight sampling policy is defect predictors learned from static code attributes.
Given software described in the attributes of \tab{ck},   data miners can
learn where the probability of software defects is highest.

The rest of this section argues that such defect predictors are   {\em easy to
use}, {\em widely-used}, and {\em useful} to use.

{\em Easy to use:} Static code attributes can be automatically collected, even for very large systems~\cite{nagappan05}.
Other methods, like  manual code reviews, are far slower and far more labor-intensive.
For example, depending on the review methods, 8 to 20 LOC/minute can be
inspected and this effort repeats for all members of the review team,
which can be as large as four or six people~\cite{me02f}. 

{\em Widely used:}  Researchers and industrial practitioners  use static attributes to guide software 
quality predictions.
 Defect prediction models have been reported
  at Google~\cite{lewis13}.
Verification and validation (V\&V) textbooks
(\cite{rakitin01}) advise using static code complexity attributes
to decide which modules are worth manual inspections.  


{\em Useful:}
Defect predictors often  find the location of  70\% (or more)
of the defects in code~\cite{me07b}.
Defect predictors have some level of generality:
predictors learned at NASA~\cite{me07b} have also been found useful elsewhere
(e.g. in Turkey~\cite{tosun10,tosun09}.
The success of this method in  predictors in finding bugs is   markedly
higher than other currently-used
industrial
methods such as manual code reviews. For example, 
a  panel at {\em IEEE Metrics
2002}~\cite{shu02} concluded that manual software  reviews can find ${\approx}60\%$ 
of defects.
In other work, 
Raffo documents the typical    defect detection capability of
industrial review methods:   around 50\%
 for full Fagan inspections~\cite{fagan76} to
21\% for less-structured inspections.

Not only do static code defect predictors perform well compared to manual methods,
they also are competitive with certain automatic methods.
A recent study at ICSE'14, Rahman et al.~\cite{rahman14:icse} compared
(a) static code analysis tools FindBugs, Jlint, and Pmd and (b)
static code defect predictors
(which they called ``statistical defect prediction'') built using logistic regression.
They found  no significant differences in the cost-effectiveness
of these  approaches. Given this equivalence, it is significant to note that 
static code defect prediction can be quickly adapted to new languages by building lightweight
parsers that find   information like \tab{ck}. The same is not true for   static code analyzers-- these need  extensive modification before they can be used on new
languages.

