This section describes this study's learners (CART~\cite{brieman00}, Random Forest~\cite{breiman84}, 
and WHERE~\cite{menzies2013local}) and their
tuning parameters (summarised in \fig{parameters}).

Our implementations
for CART and Random Forest comes from 
SciKitLearn~\cite{scikit-learn}.
WHERE is available from
github.com/ai-se/where\footnote{\wei{ need a nice  where repo. with an
data and code examples sub-directory. clean code. throw out anything not needed. before april1.}}.
WHERE has the most tuning parameters. This turns out
to be important since, in the experiments shown below, tuned WHERE 
out-performed the other learners-- a result suggesting that
if you are going to tune learners, then use one with many tuning options.

%%%%%%%%%%%%%%%% list of parameters%%%%%%%%%%%%%%%%%%%%%
\renewcommand\arraystretch{1.2}
\begin{figure*}[t!]
\scriptsize
  \centering
	\begin{tabular}{|c|c|c|c|l|}
	\cline{1-5}
	\begin{tabular}[c]{@{}c@{}}Learner \\ Name\end{tabular} & Parameters & Default &\begin{tabular}[c]{@{}c@{}}Tuning\\ Range\end{tabular}& 
\multicolumn{1}{c|}{Description} \\ \hline
	\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}Where-based\\ Learner\end{tabular}} 
	& threshold & 0.5 &[0.01,1]& The value to determine defective or not .\\ \cline{2-5} 
	& infoPrune & 0.33 &[0.01,1]& The percentage of features to consider  for the best 
split to build CART tree\footnote{Since the Where-based learner will build two trees, the first 
one is for clustering and the second one is building prediction model. we explicitly call Where-
clustering tree and CART tree, respectively}. \\ \cline{2-5} 
	 & min\_sample\_split & 4& [1,10]& The minimum number of samples required to split an internal node of
CART tree. \\ \cline{2-5} 
	 & min\_Size & 0.5 &[0.01,1]& \begin{tabular}[c]{@{}l@{}}The value to determine the minimum 
number of samples to be a Where-clustering tree \\ based on  ${n\_samples}^ {min\_Size}$.
\end{tabular} \\ \cline{2-5} 
    & wriggle & 0.2 &[0.01, 1] & The threshold to determine which branch in  Where tree to be pruned\\ \cline{2-5}
	 & depthMin & 2 & [1,6]&The minimum depth of the tree below which no pruning for Where-
clustering tree. \\ \cline{2-5} 
	 & depthMax & 10 &[1,20]& The maximum depth of the Where-clustering tree. \\ \cline{2-5} 
	 & wherePrune & False &T/F& Whether or not to prune the Where-clustering tree. \\ \cline{2-5}
	 & treePrune & True &T/F& Whether or not to prune the classification tree built by CART. \\ \cline{2-5} 
\hline
\multirow{4}{*}{CART} & threshold & 0.5 &[0,1]& The value to determine defective or not. \\ \cline{2-5} 
	 & max\_feature & None &[0.01,1]& The number of features to consider when looking for the best 
split. \\ \cline{2-5} 

	 & min\_sample\_split & 2 &[2,20]& The minimum number of samples required to split an 
internal node. \\ \cline{2-5} 
	 & min\_samples\_leaf & 1 & [1,20]&The minimum number of samples required to be at a leaf 
node. \\ \cline{1-5}  
       \multirow{5}{*}{\begin{tabular}[c]{@{}c@{}}Random \\ Forests\end{tabular}}  & threshold & 0.5 & [0.01,1] & The value to determine defective or not. \\ 
\cline{2-5} 
	 & max\_feature & None &[0.01,1]& The number of features to consider when looking for the best 
split. \\ \cline{2-5} 
	 & max\_leaf\_nodes & None &[1,50]& Grow trees with max\_leaf\_nodes in best-first fashion. \\ \cline{2-5} 
	 & min\_sample\_split & 2 &[2,20]& The minimum number of samples required to split an 
internal node. \\ \cline{2-5} 
	 & min\_samples\_leaf & 1 &[1,20]&The minimum number of samples required to be at a leaf 
node. \\ \cline{2-5} 
	 &  n\_estimators & 100 & [50,150]&The number of trees in the forest.\\ \cline{2-5}
	 \hline

	\end{tabular}
    \caption {List of parameters to be tuned.}
\label{fig:parameters}
\end{figure*}
 
\subsection{Why Study These Algorithms?}


This paper studies WHERE since this was the first learner we tried to tune and, as shown below,
it offers an interesting case study on the benefits of tuning.

This paper also studies  CART and Random Forest since  these were used in 
a recent IEEE TSE paper by Lessmann et al.~\cite{lessmann2008benchmarking} that compared 21 different 
learners for software defect prediction:
\bi
\item
{\em Statistical classifiers:}
Linear    discriminant analysis,
Quadratic discriminant analysis,
Logistic regression,
Naive Bayes,
Bayesian networks,
Least-angle regression,
Relevance vector machine,

\item
{\em Nearest neighbor methods:}
k-nearest neighbor,
K-Star

\item
{\em Neural networks:}
Multi-Layer Perceptron,
Radial bias function network,

\item
{\em Support vector machine-based classifiers:}
Support vector machine,
Lagrangian SVM
Least squares SVM,
Linear programming,
Voted perceptron,

\item
{\em Decision-tree approaches:}
C4.5 decision tree,
CART,
Alternating decision tree.
\item
{\em Ensemble methods:}
Random Forest,
Logistic Model Tree.
\ei
In that study, CART was severely trounced (was ranked last) and Random Forest was
the standout best method. The experiments shown below both confirm and refute
that ranking. In a result consistent with the prior result, untuned Random Forest performs best.
However, after tuning, the worst learner found by Lessmann et al. (CART) performed better
than Random Forest.
  

\subsection{Learners and Their Tunings}

CART, Random Forest, and WHERE are all  tree learners that divide a data set, then recurse
on each split.
If data contains more than {\em min sample split}, then a split is attempted.
On the other hand, if a split contains no more than {\em min samples leaf}, then recursion stops. For CART and Random Forest use a 
user-supplied constant for this parameter while
WHERE computes $m$={\em min samples leaf} from the size of the data
sets via  $m=\mathit{size}^\mathit{min size}$ (so, for WHERE,
{\em min size} is the parameter to be tuned).

These learners
generate numeric predictions which are converted
into binary ``yes/no'' decisions via \eq{yesno}. Hence, they all use the {\em threshold} value $T$ discussed in \tion{eg}.

These learners use different techniques to explore the splits:
\bi
\item
CART finds the attributes whose ranges contain rows with least variance in the number
of defects\footnote{If an attribute ranges $r_i$ is found in 
$n_i$ rows each with a  defect count variance of $v_i$, then CART seeks the attributes
whose ranges minimizes $\sum_i \left(\sqrt{v_i}\times n_i/(\sum_i n_i)\right)$.}.
\item
Random Forest    divides data like CART,
but it builds $F>1$  trees, each time with a subset of
the attributes (selected at random). 
\item
WHERE projects the data on to a dimension it synthesizes from the raw data using
a process analogous to principle component analysis\footnote{
PCA  synthesises  new
attributes $e_i, e_2,...$
that extends across the dimension of greatest  variance in the data  with attributes $d$.  
This process  combines
redundant  variables into a smaller set of variables  (so $e \ll d$) since those
redundancies become (approximately) parallel lines
in $e$ space. For all such redundancies \mbox{$i,j \in d$}, we 
can ignore $j$ 
since effects that change over $j$ also
change in the same way over $i$.
PCA is also useful for skipping over noisy variables from $d$-- these
variables are effectively ignored since    they  do not contribute to the variance in the data.}.
WHERE   divides  at the median point of that projection. On recursion,
this generates a dendogram, the leaves of which are clusters of  very similar examples.
\ei
WHERE's {\em infoPrune} tuning parameter then choices the
attributes   that best select  different clusters.
WHERE pretends its clusters are ``classes'', then 
asks the InfoGain of the
Fayyad-Irani discretizer~\cite{FayIra93Multi}, to rank the attriubutes.
WHERE then ignores everything except the top   {\em infoPrune} percent of the sorted
attributes.

Optionally, if the {\em where prune} option is set, 
WHERE  continues to applies infogain criteria  recursively to build a tree that selects for the
different clusters. If WHERE's {\em tree prune} parameter is enabled, then WHERE also prunes  superfluous sub-trees. For example, if a sub-tree and its parent have the same 
majority cluster
(one that occurs most frequently), then we prune the sub-tree.
This tree pruning  sometimes
prunes aways all  cluster selectors branches. To tame this effect, the {\em wriggle} parameter
blocks tree pruning for at least the first {\em wriggle} number of initial branches.

Other tuning parameters are learner specific. For example,
{\em max feature} is used by
CART and Random Forest to select the number of attributes
used to build one tree.
CART's default is to use all the attributes while 
Random Forest usually selects the square root of the number
of attributes.
Also,
  {\em max leaf nodes} is the upper bound on leaf notes generated in a 
  Random Forest.



\subsection{Tuning Algorithms}


 \subsubsection{Parametric Tuning Algorithms}
The  goal of this paper is to adjust the tuning parameters of \fig{parameters}
in order to   optimize (improve) some particular performance scores
generated by a particular learner being applied to  a particular data set.
For this task, we do not use traditional parametric numeric optimizer  
such as  gradient descent optimizers~\cite{saltelli00} that require models comprise
differential functions (i.e. functions of real-valued variables whose derivative exists at each point in its domain).
This is impractical  for  our learners since their internal states are   not a smoothly differential continuous function.
Rather, learners being tuned  contains many regions with many different properties (tuning options can
drive the learner into very different modes with very different performance properties).


 \subsubsection{Non-Parametric Tuning Algorithms}
 
Non-parametric  optimizers   make no assumption
about the model being only differential functions. One such optimizer
is simulated annealing. SA generates {\em new} solutions
 by randomly perturbing (a.k.a. ``mutating'') some part of an {\em old}
 solution.  {\em New} replaces {\em old} if (a) it scores higher; or
 (b) it reaches some probability set by a ``temperature'' constant. Initially,
 temperature is high so SA jumps to sub-optimal solutions (this allows
 the algorithm to escape from local minima). Subsequently, the
 ``temperature'' cools and SA only ever moves to better {\em new}
 solutions. 
 SA is often used in search-based SE
 e.g.~\cite{fea02a,me07f}, perhaps due to its simplicity.

SA was invented   in the 1950s, when
 computer RAM was very small~\cite{kirkpatrick83}. A standard SA algorithm needs
 only space for three solutions {\em new, old} and the {\em best} seen so far.
  In the 1960s, when more RAM became available, it became standard to
 generate many {\em new} mutants, and then combine together parts of
 promising solutions~\cite{goldberg79}.  Such {\em evolutionary
   algorithms} (EA) work in {\em generations} over a population of
 candidate solutions.  Initially, the population is created at random.
 Subsequently, each generation makes use of select+crossover+mutate
 operators to pick promising solutions, mix them up in some way, and
 then slightly adjust them.
 EAs are also often used in search-based
 software engineering, particularly in test case generation~\cite{andrews07,andrews10}
 or refactoring~\cite{Weimer:2009}

 Later work focused on creative ways to control the
 mutation process. Tabu search and scatter search
 work to bias new mutations away from prior
 mutations~\cite{Glover1986563,Beausoleil2006426,Molina05sspmo:a,4455350}.
 Particle swarm
 optimization randomly mutates multiple solutions
 (which are called ``particles''), but biases those
 mutations towards the best solution seen by one
 particle and/or by the neighborhood around that
 particle~\cite{pan08}.
 Differential evolution mutates solutions by
 interpolating between members of the current
 population~\cite{storn1997differential}.  
 
Another more recent technique that has claimed much attention
are   heuristics that decompose the total space into many smaller problems, and then which use a simpler optimizer for each region. 
For example, in $\mathcal{E}$-domination~\cite{deb05}, the  user is asked
`what is the lower threshold $\mathcal{E}$ on the size of a useful effect?''. The solution space
is then divided into boxes of size $\mathcal{E}$ and linked such that  the  set $X.\mathit{lower}$ contains boxes with worse objective scores that $X$.  Solutions in pairs boxes are  quickly compared  using   small samples from each  and, if some box $X$ is found to be inferior, then it is quickly pruned along with all
solutions in the $X.\mathit{lower}$ boxes.
Later research generalized this approach. MOEA/D (multiobjective
evolutionary algorithm based on decomposition~\cite{zhang07}) is a generic framework that decomposes a multiobjective optimization problem into many smaller single problems, then applies a second optimizer to each smaller subproblem, simultaneously.   Other work in this arena are the response surface methods
that quickly find multiple approximations to the problem, each of which holds for a very tiny region.
Each region has a ``slope'' and examples in that region are pushed along the slope towards better
solutions~\cite{krall15,Zuluaga:13}.
 
 
\input{algo} 
 \subsubsection{Selecting a Tuning Algorithm}
 
From all the above methods, how do we select which optimizers to apply to tuning data miners.
Cohen~\cite{cohen95} advises comparing any supposedly more
sophisticated method against the simplest possible alternative. For
example, in one study with ``floor effects'', Holte showed that,
often, much of the performance of complex multi-level decision trees
could be easily achieved using a much simpler single-level decision
tree learner called 1R~\cite{holte93}. He therefore recommends a very simple rule learner
(called ``1R'') as a
kind of ``scout'' that can do a quick preliminary analysis of a data
set and which can report back if that data really requires a more
complex analysis.

To find our ``scout'',  we used engineering judgement to sort  the above algorithms from simplest to most complex.
The three simplest optimizers are SA, $\mathcal{E}$-domination, and 
differential evolution (each can be coded in less than a page of some high-level scripting language). Our reading of the current literature is that there are more  advocates for
differential evolution than
  SA or $\mathcal{E}$-domination:
  \bi
  \item
  When the MOEA/D community requires a secondary optimizer, they often use  differential evolution~\cite{zhang07,5583335}.
  \item
 Vesterstrom and Thomsen~\cite{Vesterstrom04} report that DE is competitive with 
   particle swarm optimization and a genetic algorithm. 
   \ei
DEs have been applied before for   parameter tuning (e.g. see~\cite{omran2005differential, chiha2012tuning}) but this is the first time they have been applied to
optimizing defect prediction from static code attributes.  


 
 


 
 