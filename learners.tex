There are many ways to build defect predictors
such as  CART~\cite{brieman00}, Random Forest~\cite{breiman84}, 
and WHERE~\cite{menzies2013local}.
For this study, we use the CART and Random Forest  from 
SciKitLearn~\cite{scikit-learn} (and
WHERE is available from
github.com/ai-se/where).
\fig{parameters} shows these learner's tuning options.


 We use  these algorithms, for the following reasons.
CART and Random Forest were mentioned in
a recent IEEE TSE paper by Lessmann et al.~\cite{lessmann2008benchmarking} that compared 21  
learners for  defect prediction.
That study ranked  CART  worst  and Random Forest as best.
In a demonstration of the impact of tuning,
this paper shows  we can {\em reverse} those rankings (sometimes)
such that  the worst learner found by Lessmann et al. (CART) can perform better
than Random Forest.

This
 paper also presents results with WHERE-based learner since, as shown below,
it offers an interesting case study on the benefits of tuning.
  

\subsection{Learners and Their Tunings}

CART, Random Forest, and WHERE-based learner are all  tree learners that divide a data set, then recurse
on each split.
All these learners
generate numeric predictions which are converted
into binary ``yes/no'' decisions via \eq{yesno}. Hence, they all use the {\em threshold} value $T$ discussed in \tion{eg}.

The splitting process is controlled by numerous tuning parameters.
If data contains more than {\em min\_sample\_split}, then a split is attempted.
On the other hand, if a split contains no more than {\em min\_samples\_leaf}, then recursion stops. For CART and Random Forest use a 
user-supplied constant for this parameter while
WHERE-based learner firstly computes this parameter $m$={\em min\_samples\_leaf} from the size of the data
sets via  $m=\mathit{size}^\mathit{min\_size}$ to build an initial  clustering tree.
Note that WHERE builds {\em two} trees: the initial clustering tree (to find similar sets of data)
then a final decision tree (to learn rules that predict for each similar cluster)\footnote{A
frequently asked question is why WHERE so complex?
Would not   a single tree suffice? The answer is, as shown below,  tuned WHERE's twin-tree approach 
generates very precise predictors.}.
The tuning parameter  {\em min\_sample\_ split } controls the construction of the
final decision tree (so, for WHERE-based learner,
{\em min\_size} and {\em min\_sample\_split} are the parameters to be tuned).

These learners use different techniques to explore the splits:
\bi
\item
CART finds the attributes whose ranges contain rows with least variance in the number
of defects\footnote{If an attribute ranges $r_i$ is found in 
$n_i$ rows each with a  defect count variance of $v_i$, then CART seeks the attributes
whose ranges minimizes $\sum_i \left(\sqrt{v_i}\times n_i/(\sum_i n_i)\right)$.}.
\item
Random Forest    divides data like CART then  builds $F>1$  trees,
each time using some random subset of
the attributes. 
\item
When building the initial cluster tree, WHERE projects the data on to a dimension it synthesizes from the raw data using
a process analogous to principle component analysis\footnote{
PCA  synthesises  new
attributes $e_i, e_2,...$
that extends across the dimension of greatest  variance in the data  with attributes $d$.  
This process  combines
redundant  variables into a smaller set of variables  (so $e \ll d$) since those
redundancies become (approximately) parallel lines
in $e$ space. For all such redundancies \mbox{$i,j \in d$}, we 
can ignore $j$ 
since effects that change over $j$ also
change in the same way over $i$.
PCA is also useful for skipping over noisy variables from $d$-- these
variables are effectively ignored since    they  do not contribute to the variance in the data.}.
WHERE  divides  at the median point of that projection.
On recursion,
this generates the initial clustering tree, the leaves of which are clusters of  very similar examples. After that, when building 
the final decision tree, WHERE pretends its clusters are ``classes'', then 
asks the InfoGain of the
Fayyad-Irani discretizer~\cite{FayIra93Multi}, to rank the attriubutes, where {\em infoPrune} is used.
WHERE's final decision tree generator then ignores everything except the top   {\em infoPrune} percent of the sorted
attributes.
\ei
Some tuning parameters are learner specific:
\bi
\item
{\em Max\_feature} is used by
CART and Random Forest to select the number of attributes
used to build one tree.
CART's default is to use all the attributes while 
Random Forest usually selects the square root of the number
of attributes.
\item
  {\em Max\_leaf\_nodes} is the upper bound on leaf notes generated in a 
  Random Forest.
\item {\em Max\_depth} is the upper bound on the depth of the CART tree.  
 \item
WHERE's initial clustering
tree generation will always split up to {\em depthMin} number of branches.
After that, WHERE will only split data if the mean performance scores of the two halves
is ``trivially small'' (where ``trivially small'' is set by the   {\em wriggle} parameter). 
\item
WHERE's   {\em tree\_prune} setting controls how   
WHERE prunes back superflous parts of the final decision tree. 
If a decision sub-tree and its parent have the same 
majority cluster
(one that occurs most frequently), then if {\em tree\_prune} is enabled, we prune that decision sub-tree.
\ei





\subsection{Tuning Algorithms}


 \subsubsection{Parametric Tuning Algorithms}
The  goal of this paper is to adjust the tuning parameters of \fig{parameters}
in order to   optimize (improve) some particular performance scores
generated by a particular learner being applied to  a particular data set.
For this task, we do not use traditional parametric numeric optimizer  
such as  gradient descent optimizers~\cite{saltelli00} that require models comprised of
differential functions (i.e. functions of real-valued variables whose derivative exists at each point in its domain).
This is impractical  for  our learners since their internal states are   not a smoothly differential continuous function.
Rather, learners being tuned  contain many regions with many different properties (tuning options can
drive the learner into very different modes with very different performance properties).


 \subsubsection{Non-Parametric Tuning Algorithms}
 
Non-parametric  optimizers   make no assumption
about the model being only differential functions. One such optimizer
is simulated annealing. SA generates {\em new} solutions
 by randomly perturbing (a.k.a. ``mutating'') some part of an {\em old}
 solution.  {\em New} replaces {\em old} if (a) it scores higher; or
 (b) it reaches some probability set by a ``temperature'' constant. Initially,
 temperature is high so SA jumps to sub-optimal solutions (this allows
 the algorithm to escape from local minima). Subsequently, the
 ``temperature'' cools and SA only ever moves to better {\em new}
 solutions. 
 SA is often used in search-based SE
 e.g.~\cite{fea02a,me07f}, perhaps due to its simplicity.

SA was invented   in the 1950s, when
 computer RAM was very small~\cite{kirkpatrick83}. A standard SA algorithm needs
 only space for three solutions {\em new, old} and the {\em best} seen so far.
  In the 1960s, when more RAM became available, it became standard to
 generate many {\em new} mutants, and then combine together parts of
 promising solutions~\cite{goldberg79}.  Such {\em evolutionary
   algorithms} (EA) work in {\em generations} over a population of
 candidate solutions.  Initially, the population is created at random.
 Subsequently, each generation makes use of select+crossover+mutate
 operators to pick promising solutions, mix them up in some way, and
 then slightly adjust them.
 EAs are also often used in search-based
 software engineering, particularly in test case generation~\cite{andrews07,andrews10}
 or refactoring~\cite{Weimer:2009}

 Later work focused on smarter  
 mutations. Tabu search and scatter search
 work to bias new mutations away from prior
 mutations~\cite{Glover1986563,Beausoleil2006426,Molina05sspmo:a,4455350}.
 Particle swarm
 optimization randomly mutates multiple solutions
 (which are called ``particles''), but biases those
 mutations towards the best solution seen by one
 particle and/or by the neighborhood around that
 particle~\cite{pan08}.
 Differential evolution mutates solutions by
 interpolating between items in the current
 population~\cite{storn1997differential}.  
 
 

 
 
Another approach uses
    heuristics to decompose the total space into   small problems,   then applies a
    simpler optimizer to each region. 
In $\mathcal{E}$-domination~\cite{deb05}, the  user is asked
`what is the lower threshold $\mathcal{E}$ on the size of a useful effect?''. The solution space
is then divided into boxes of size $\mathcal{E}$ and linked such that  the  set $X.\mathit{lower}$ contains boxes with worse objective scores that $X$.  Solutions in pairs boxes are  quickly compared  using   small samples from each  and, if some box $X$ is found to be inferior, then it is quickly pruned along with all
solutions in the $X.\mathit{lower}$ boxes.
Later research generalized this approach. MOEA/D (multiobjective
evolutionary algorithm based on decomposition~\cite{zhang07}) is a generic framework that decomposes a multiobjective optimization problem into many smaller single problems, then applies a second optimizer to each smaller subproblem, simultaneously.   Other work in this area are the response surface methods
that quickly find multiple approximations to the problem, each of which holds for a very tiny region.
Each region has a ``slope'' and examples in that region are pushed along the slope towards better
solutions~\cite{krall15,Zuluaga:13}.
 
 
 \subsubsection{Selecting a Tuning Algorithm}
 
From all the above methods, how do we select which optimizers to apply to tuning data miners?
Cohen~\cite{cohen95} advises comparing any supposedly more
sophisticated method against the simplest possible alternative. For
example, in one study with ``floor effects'', Holte showed that,
often, much of the performance of complex multi-level decision trees
could be easily achieved using a much simpler single-level decision
tree learner called 1R~\cite{holte93}. He therefore recommends a very simple rule learner
(called ``1R'') as a
kind of ``scout'' that can do a quick preliminary analysis of a data
set and which can report back if that data really requires a more
complex analysis.

To find our ``scout'',  we used engineering judgement to sort  the above algorithms from simplest to most complex.
The three simplest optimizers are SA, $\mathcal{E}$-domination, and 
differential evolution (each can be coded in less than a page of some high-level scripting language). Our reading of the current literature is that there are more  advocates for
differential evolution than
  SA or $\mathcal{E}$-domination:
  \bi
  \item
  When the MOEA/D community requires a secondary optimizer, they often use  differential evolution~\cite{zhang07,5583335}.
  \item
 Vesterstrom and Thomsen~\cite{Vesterstrom04} report that DE is competitive with 
   particle swarm optimization and a genetic algorithm. 
   \ei
DEs have been applied before for   parameter tuning (e.g. see~\cite{omran2005differential, chiha2012tuning}) but this is the first time they have been applied to
optimize defect prediction from static code attributes.  


 
 

 