This section describes the learners used in this study: CART~\cite{brieman00}, Random Forest~\cite{breiman84}, 
and WHERE~\cite{menzies2013local}. The implementations
for CART and Random Forest comes from 
SciKitLearn~\cite{scikit-learn}.
WHERE is available from
github.com/ai-se/where\footnote{\wei{ need a nice  where repo. with an
data and code examples sub-directory. clean code. throw out anything not needed. before april1.}}

CART, Random Forest, and WHERE are all  tree learners that divide a data set, then recurses
on each split.
All our learners use the following tuning parameters to control that splitting process.
These learners
generate numeric predictions which are converted
into binary ``yes/no'' decisions via \eq{yesno}. Hence, they all use the threshold value $T$ discussed in \tion{eq}.
Also, if data contains more than {\em min sample split}, then a split is attempted.
On the other hand, if a split contains no more than {\em min samples leaf}, then recursion stops. For CART and Random Forest use a 
user-supplied constant for this parameter while
WHERE computes $m$={\em min samples leaf} from the size of the data
sets via  $m=\mathit{size}^\mathif{min size}$ (so, for WHERE,
{\em min size} is the parameter to be tuned).

These learners use different techniques to explore the splits:
\bi
\item
CART finds the attributes whose ranges contain rows with least variance in the number
of defects\footnote{If an attribute ranges $r_i$ is found in 
$n_i$ rows each with a  defect count variance of $v_i$, then CART seeks the attributes
whose ranges minimizes $\sum_i \left(\sqrt{v_i}\times n_i/(\sum_i n_i)\right)$.}.
\item
Random Forest    divides data like CART,
but it builds more than one tree, each time using some subset of
the attributes (selected at random). 
\item
WHERE projects the data on to dimensions synthesizes from the raw data using
a process analogous to principle component analysis\footnote{
PCA  synthesises  new
attributes $e_i, e_2,...$
that extends across the dimension of greatest  variance in the data  with attributes $d$.  
This process  combines
redundant  variables into a smaller set of variables  (so $e \ll d$) since those
redundancies become (approximately) parallel lines
in $e$ space. For all such redundancies \mbox{$i,j \in d$}, we 
can ignore $j$ 
since effects that change over $j$ also
change in the same way over $i$.
PCA is also useful for skipping over noisy variables from $d$-- these
variables are effectively ignored since    they  do not contribute to the variance in the data.}.
WHERE   divides  at the median point of that projection. On recursion,
this generates a dendogram, the leaves of which are clusters of  very similar examples.
\ei
WHERE's {\em infoPrune} tuning parameter then uses  what attributes are used in this process.

XXX

Once WHERE's dengogram is generated, some mechanism must be used to map test cases into specific 
clusters. To this end, WHERE pretends its clusters are ``classes'' and 
a entropy-based decision tree learner   to find
the decisions that best select for each ``class'' (i.e. each cluster).
This entropy-based learner  finds the attributes that build sub-trees that best select
for the fewest  clusters\footnote{In entropy based decision-tree learning,  the
the splitting attribute is the one that most  minimizes $\sum_i \left( e_i\times n_i/(\sum_i n_i)\right)$,
defined as follows.
Let    attribute ranges $r_i$ select
$n_i$ rows that are found in $c_j$ clusters.  Then the probability of
arriving at those clusters is $p_j = c_j/n_i$ and the entropy
of those rows is $e_i = \sum_j \left( -p_j \ldot log_2p_j\right)$.}.
\ei
After b

If WHERE's {\em tree prune} parameter is enabled, then WHERE also prunes  superfluous sub-trees
in the cluster-selection trees. For example, if a sub-tree and its parent have the same 
majority cluster
(one that occurs most frequently), then we prune the sub-tree.





Other tuning parameters are learner specific. For example,
{\em max feature} is used by
CART and Random Forest to select the number of attributes
used to build one tree.
CART's default is to use all the attributes while 
Random Forest usually selects the square root of the number
of attributes.
Also,
$F$ is the number of trees built by Random Forest 
(and was
  discussed in our introduction).
 Further,
  {\em max leaf nodes} is the upper bound on leaf notes generated in a 
  Random Forest.

Of all our learners, WHERE has the most tuning parameters. This turns out
to be significant since, in the experiments shown below, tuned WHERE 
out-performed the other learners. This suggests the heuristic that if
you are going to tune learners, use one with the most tuning options.

After building its tree, WHERE ranks all attributes by how well
they select for fewest leaves (using the  infogain measure
from the Fayyad-Irani discretizer~\cite{FayIra93Multi}, where ``class'' is really the leaf cluster ids). It then only uses the best {\em infoPrune} percent of the attributes to build
its cluster selection tree. Also, the {\em tree prune} parameter (described above) sometimes
prunes aways all  cluster selectors branches. To tame this effect, the {\em wriggle} parameter
blocks tree pruning for at least the the   {\em wriggle} number of initial branches.



We choose CART and Random Forest since  these were used in 
a recent IEEE TSE paper by Lessmann et al.~\cite{lessmann2008benchmarking} that compared 21 different 
learners for software defect prediction:
\bi
\item
{\em Statistical classifiers:}
Linear    discriminant analysis,
Quadratic discriminant analysis,
Logistic regression,
Naive Bayes,
Bayesian networks,
Least-angle regression,
Relevance vector machine,

\item
{\em Nearest neighbor methods:}
k-nearest neighbor,
K-Star

\item
{\em Neural networks:}
Multi-Layer Perceptron,
Radial bias function network,

\item
{\em Support vector machine-based classifiers:}
Support vector machine,
Lagrangian SVM
Least squares SVM,
Linear programming,
Voted perceptron,

\item
{\em Decision-tree approaches:}
C4.5 decision tree,
CART,
Alternating decision tree.
\item
{\em Ensemble methods:}
Random Forest,
Logistic Model Tree.
\ei
In that study, CART was severely trounced (was ranked last) and Random Forest was
the standout best method. The experiments shown below both confirm and refute
that ranking. In a result consistent with the prior result, untuned Random Forest performs best.
However, after tuning, the worst learner found by Lessmann et al. (CART) performed better
than Random Forest.

, where the performance of both tools as predictors are 
significantly different based on authors' experiment as mentioned before. We'd like to 
investigate whether tuning can change such conclusion. WHERE-based Predictor is a new 
defect predictor based on WHERE\cite{menzies2013local} algorithm. A comparison with 
standard predictor like CART and Random Forest will better evaluate and judge the 
performance such new predictor. As for the tuner, there're many heuristic optimization 
algorithms in wild. However, DE is a good but maybe not the best candidate for the tuning 
process considering different performance measurements. To determine which optimizer is 
fitable and results in better performance is beyond this work scope. We leave it to future 
works. The rest of this section will describe each tool applied in this work.



While there are many technical
differences
in those learners, they have certain similarities.

To begin with, all the learners in this study generate numeric predictions which are converted
into binary ``yes/no'' decisions via \eq{yesno}. Hence, they all have some threshold  value $T$ 
that can be tuned.


same goal: to summarize a large data set into a small model\footnote{Technical note: proponents of instance-based reasoning might argue that
their goal is to retain, and not summarize, the training data.
However, 
even instance-based reasoners use some degree of summarizing when they learn
ways to bias the instance matching away from irrelevant training data.
Those bias tools include  generalizing points to ranges (to allow for partial matches)
or  clustering algorithm that restrict the case matching
to just the subset of old cases most similar  to new  cases.}. 

For example,
consider some learner that
builds predictive rules from a subset of  attributes $A$ ranges. Assuming numeric attributes
divided into $d=5$ divisions, then the 20 attributes of \fig{ck} can be combined into 
$2^{A*d} = 2^{5*20} > 10^{30}$
possible rules. The key thing to note here is that many of these rules will have similar effects
since, at least in software engineering, data sets containing rows $R$ and columns $C$ can contain much redundancy:
\bi
\item When $c \subset C$ columns of data that are correlated to each other, then those
$c$ columns can be replaced by one column.
\item When $r \subset R$ rows of data are very similar (e.g. they come from similar projects), 
then those $r$ rows can be replaced
by one cluster centroid that stands in $r$.
\item
The seperate experiments of
Kocagenunil~\cite{me13a} and Papakroni~\cite{papa13}  show that  tables  of data
like \fig{ck} can be be pruned back to around
25\% of the columns and 10\% of the rows without losing the essential information in the data set.
This reduced space contains $0.25*0.1 = 2.5\%$ of the cells in the original table.
While some of that reduction is due to the removal of noise, their conclusion is correlated
columns and similar rows are very common in SE data sets.
\ei
Given this high level of redundancy, learners need only explore a small fraction of these $2^{A*d}$ rules.
Most learners exploit this fact to optimize their reasoning. Rather than explore all options,
they employ some {\em search bias} that lets them quickly focus on a small set of promising rules.

Different biases work best for different data sets so it is standard practice to allow an analyst
to control these biases via tuning parameters. For example, consider the search biases
$B_i$ of decision tree learner that :
\bi
\item Splits the data on some criteria $B_1$;
\item Assesses each split using some criteria $B_2$;
\item Selects which split(s) to explore further using $B_3$;
\item Recurses into each split until some stopping criteria $B_4$ is encountered.
\ei
These biases also appear when the learned decision tree makes a decision:
\bi
\item To declare a software module to be XXX
\ei


\section{Algorithm: Predictor and Tuner}

In order to conduct the experiment of this paper, we need one tool that can predict defects 
from the empirical data sets and a second tool to tune the built-in parameters associated with 
predictor. To compare the effects of the tuning process, we have three different predictors: 
WHERE-based Predictor, CART and Random Forest. Different Evolution(DE) as an optimizer 
is used as a tuner in this paper.

We choose CART and Random Forest as a predictor in this paper is motivated by 
\cite{lessmann2008benchmarking}, where the performance of both tools as predictors are 
significantly different based on authors' experiment as mentioned before. We'd like to 
investigate whether tuning can change such conclusion. WHERE-based Predictor is a new 
defect predictor based on WHERE\cite{menzies2013local} algorithm. A comparison with 
standard predictor like CART and Random Forest will better evaluate and judge the 
performance such new predictor. As for the tuner, there're many heuristic optimization 
algorithms in wild. However, DE is a good but maybe not the best candidate for the tuning 
process considering different performance measurements. To determine which optimizer is 
fitable and results in better performance is beyond this work scope. We leave it to future 
works. The rest of this section will describe each tool applied in this work.

 \subsection{WHERE-based Learner}
\textbf{WHERE-based Learner} is composed of WHERE clustering algorithm and CART 
decision tree algorithm. The key idea of WHERE-based learner is that  instead of training the 
CART decision tree based on the class labels associated with each training sample, it's using 
CART to build decision trees based on the cluster labels, which are generated by the WHERE 
clustering algorithm.
 
 
 
\begin{figure}[!t]

\renewcommand{\baselinestretch}{0.8}
\scriptsize
\centering
  \begin{tabular}{r@{~}|r@{~}rr|r@{~}rr|r@{~}rr}
    & \multicolumn{3}{c|}{WHERE}&\multicolumn{3}{c|}{CART}& \multicolumn{3}{c}{Random Forest}\\\cline{2-10} 
    Goal & tuned & naive &$\frac{\mathit{tuned}}{\mathit{naive}}$& tuned & naive &$\frac{\mathit{tuned}}{\mathit{naive}}$& tuned & naive&$\frac{\mathit{tuned}}{\mathit{naive}}$\\\hline
pd&71.4&1.2&60&4.4&0.1&44&7.1&0.2&36\\ 
prec&82.8&1.4&59&3.9&0.1&39&7.4&0.2&37\\
f&86.1&1.2&72&3.3&0.1&33&6.7&0.2&34
  \end{tabular}
  \caption{For one train/test set pair (ANT),  time (in seconds) spent tuning,
  for different goals, or just running the ``off-the-shelf'' learner.}
\end{figure}


\begin{figure}[!t]

\renewcommand{\baselinestretch}{0.8}
\scriptsize
\centering
\begin{tabular}{r|rrr}
goal &	WHERE	&CART&	Random Forest\\\hline
f	&15	&13	&9\\ 
pd	&15	&16	&8\\ 
prec&	14	&14	&8
\end{tabular}
\caption{How much slower is tuning vs just running the default
settings of the learner? E.g. bottom right, tuning to increase
precision with Random Forests is eight times slower than
just running Random Forests.}
\end{figure}



% Given N instances, instead of building decision tree directly with CART, we use WHERE to 
%cluster instances. 
WHERE is a fast clustering algorithm designed by {\it menzies} for finding software artifacts 
with similar attributes. It clusters data on dimensions synthesized along the axis of greatest 
variability in the data. The way WHERE used to find such dimension is a linear-time heuristic 
called ``FASTMAP" proposed by Faloutsos \& Lin\cite{faloutsos1995fastmap}. ``FASTMAP" 
randomly picks one instance $Z$; find the instance {\it east} $X$ that's furthest away form $Z$; find 
the instance {\it west} $Y$ that's furthest away from {\it east} $X$. Next, project all the remaining 
points onto the line drawn between $X$ and $Y$.  The line $\overline{XY}$ is an approximation of the 
first component found by PCA. As shown in Figure \ref{Where}, $X$ and $Y$ are the furthest points found by ``FASTMAP'' and $\overline{XY}$ is of length $c$. Each point in this figure has a distance $a$
to $X$ and distance $b$ to $Y$. According to the coisne rule and Pythagoras, each instance will be mapped 
into 2-dimension by the following equations.
\begin{equation}
\begin{split}
x  &= (a^2 +c^2-b^2)/(2c)\\ 
   y  &= \sqrt{a^2 -x^2}
\end{split}
\end{equation}


Then choose the median point $\hat{x}$ as the split and 
recursively divide all the instances into {\it west} and {\it east} clusters until the number of 
instances within each cluster is less than specific minimum size. The representation of the 
clustering result is a tree, we call it Where-clustering tree. Such tree will be pruned if applicable 
in some cases. Finally, cluster labels  will be assigned to instances in each of those 
clusters(leaves). 

\begin{figure}[!ht]
\includegraphics[scale=0.5]{where.png}
\caption{WHERE algoirthm }
\label{Where}
\end{figure}
Then based on the attributes and the cluster labels in each instances,  we build a 
decision tree based on CART algorithm. In each leaf of the tree, there're several instances 
falling in. This will be the model trained for defect prediction. During testing process, a new 
instance comes in and traverses the tree according to 
values of the its attributes. If the instance could reach one leaf of the tree, the predicted value 
of this instance will be the mean  of all the training data values associated with this leaf. 
Otherwise if stops at intermediate nodes of tree, the predicted value will be the mean of the all 
the training data value below this node. After estimating all the number of defectives in the test 
data, whether each instance contains defectives will be determined by comparing with a threshold value. If the predicted value is greater than threshold, the Where-based learner will predict this instance as defective otherwise non-defective.


%\begin{algorithm}
%	\caption{Pesudocode for WHERE Algorithm }
%	\label{alg:WHERE}
%	\begin{algorithmic}[1]
%	\Require $data$
%	\Ensure $tree$
%	\Function {Fastmap}{$ data$}
%	\State $one \gets$ any$(data)$
%	\State $west \gets$ furthest$(data, one)$
%	\State $east \gets$ furthest$(data, west)$
%	\State $c \gets$ distance($west, east$)
%	\EndFunction
%	\end{algorithmic}            
%\end{algorithm}

\subsection{CART}
\textbf{CART} is an {\em iterative dichotomization} algorithm
that finds the attribute that most divides the data such that
the variance of the goal variable in each division is
minimized\cite{breiman84}.
The algorithm then recurses on each division.
Finally, the cost data in the leaf divisions
is averaged to generate the estimate.

\subsection{Random Forests}

Breiman's website describes \textbf{Random Forests} as
follows\cite{brieman00}. "Random Forests grows many classification
trees. To classify a new object from an input vector, put the input
vector down each of the trees in the forest. Each tree gives a
classification, and we say the tree "votes" for that class. The forest
chooses the classification having the most votes (over all the trees
in the forest). Each tree is grown as follows.
If the number of cases in the training set is N, sample N cases at
random - but with replacement, from the original data. This sample
will be the training set for growing the tree.
Also, if there are M input variables, a number m$<<$M is specified such
that at each node, m variables are selected at random out of the M and
the best split on these m is used to split the node. The value of m is
held constant during the forest growing.
Finally, each tree is grown to the largest extent possible (there is
no pruning)."




%Suppose the cordinates of X and Y are $(0,0)$ and $(0,c)$, which are east and west 
%instances within N samples. From the Pythegoras and cosine rule, each instance is projected 
%at the point(x,y)
%
%\begin{equation}
%	\begin{split}
%	x &= (a^2 + c^2 - b^2)/(2c) \\
%	y &= \sqrt{a^2 - x^2} 
%	\end{split}
%\end{equation}

