

\documentclass{acm_proc_article-sp}
\usepackage{multirow}
\usepackage{color}
\usepackage{colortbl}
\usepackage{picture}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\quart}[3]{\begin{picture}(100,6)%1
{\color{black}\put(#3,3){\circle*{4}}\put(#1,3){\line(1,0){#2}}}\end{picture}}


\begin{document}
\title{Tuning Learners before Applying: New Reflection in Defect Prediction }
\author{ A, B, C}
\maketitle
\begin{abstract}
Data mining techniques have been widely applied to software defect prediction with various 
empirical data sets. Those proposed leaners are always evaluated against the state of the art 
predictors by performing statistical analysis. Undoubtedly, given the bias from data sets and 
accuracy indicators, the newer predictors outperform counterparts in selected experimental 
settings. However,  most of data mining algorithm based predictors (e.g. CART, random 
forest, 
neural networks, SVM) have built-in {\it magic} parameters, like the number of trees in random 
forest algorithm. the impact of internal parameters in those methods have been neglected 
during evaluation. In this paper, we investigate this impact by tuning parameters in defect 
predictors with search-based software engineering algorithm. Specifically, we used differential 
evolution to tune the CART and a new predictor based on WHERE algorithm with local data, 
and then predictors with optimal parameters obtained from tuning process will be applied to 
predict defects. By comparing the performance of predictors with and without tuning process,  
we observe that tuning improves the predictors' performance and predictors woking with 
different data sets need different parameters. Our results also suggest that we should not use 
the predictors of the shelf with their default parameters and tuning should be a processor 
combined with any predictor with built-in parameters.


%RQ1: Does tuning affect learners' performance?
%RQ2: How to choose 

\end{abstract}

% A category with the (minimum) three required fields
\category{D.2.9}{Software Engineering}{Management}[Cost estimation]

\terms{Experimentation, Algorithm}

\keywords{Defect prediction, Data Mining, Tuning Parameters, CART, WHERE} 
% NOT required for Proceedings

\section{Introduction}

Software has becoming a large and complex system and delivering reliable and quality 
software is imperative for development teams. Empirical study shows that the longer the 
defects exist in software systems, the more the cost of time and money it will take to fix it 
\textbf{ [need a ref]}. Therefore, project managers and software programers strive to find 
defects in their system as early as possible. Defect prediction has been investigated 
extensively in industrial and academia during the past two decades. As an important research 
field, building data miners  \cite{lessmann2008benchmarking, mccabe1976complexity, 
menzies2007data, menzies2010defect, jiang2008can, menzies2011local, song2011general} 
over static code features of software system has been demonstrated to be a way to predict 
which models are more likely to contain defects.

Classification is an important approach to predict whether some modules in the projects are 
defective or non-defective. The general idea is to train the learners by using parts of data 
sets(e.g. ant 1.3, 1.4 in PROMISE\footnote{http://openscience.us/repo/}) and predict with 
remaining ones(ant 1.5, 1.6 and 1.7). Many types of defect predictors have been proposed 
based an different data mining classifiers, including CART, Random Forest 
\cite{guo2004robust},  Naive Bayes\cite{menzies2007data},Logistic Regression 
\cite{khoshgoftaar1999logistic}. During the past years, authors claimed that their new 
defective learner outperformed others according to their experiment and statistical analysis. 
To evalute those learners objectively in terms of accuracy, Lessmann et al
\cite{lessmann2008benchmarking} carried out a study to compare 22 classifiers over 10 public 
domain data sets from the NASA Metrics Data repository. By using Nemenyi's post hoc test 
with $\alpha = 0.05$, they concluded that the predictive accuracy of most learners didn't differ 
significantly in terms of the area under the receiver operating characteristics curve(AUC). 
Furthermore, according to the fig.2 in \cite{lessmann2008benchmarking}, Random Forest is 
significantly better than CART. Lessmann's paper motivates us to investigate whether tuning 
those CART's parameters by search-based software engineering method can improve the 
performance. Even though Lessmann considered unpruned tree and pruned tree, They didn't 
consider other possible parameters in CART which would have impact on the structure of 
trees, like the depth of the tree, the maximum and minimum number of leafs of the tree.

Software metrics are the core of all the defective prediction model. Many types of metics
are used to build models, like process metrics, McCabe and Halsted metrics  and CK metrics.
By building prediction modes across 85 releases of 12 open source projects, Rahman et al
\cite{rahman2013how}  concluded that code metrics are generally less useful than process
metrics for prediction. And also the code metrics don't change much from release to release
and lead to stagnation in the prediction model. In \cite{Radjenovi?20131397}, Radjenovi? et al
\cite{Radjenovi?20131397} reviewed 106 papers regarding software prediction metrics. They found
that CK objected-oriented and process metrics have been reported to be more successful in
finding defects compared to traditional size and complexity metrics. Moreover, not all the CK
metrics perform well equally. The best metrics from CK are CBO, WMC and RFC based on their 
observation. It seems that the relationship between software metrics and defective prediction is
still an open question and need to be addressed. This motivates us to see : whether the impact
rankings of those metrics will change after tuning parameters
is applied to model learners.



What's the problem in those result?

RQ:

briefly describe our study and  our result; observation

structure of this paper.


\section{Algorithm: Predictor and Tuner}

In order to conduct the experiment of this paper, we need one tool that can predict defects 
from the empirical data sets and a second tool to tune the built-in parameters associated with 
predictor. To compare the effects of the tuning process, we have three different predictors: 
WHERE-based Predictor, CART and Random Forest. Different Evolution(DE) as an optimizer 
is used as a tuner in this paper.

We choose CART and Random Forest as a predictor in this paper is motivated by 
\cite{lessmann2008benchmarking}, where the performance of both tools as predictors are 
significantly different based on authors' experiment as mentioned before. We'd like to 
investigate whether tuning can change such conclusion. WHERE-based Predictor is a new 
defect predictor based on WHERE\cite{menzies2013local} algorithm. A comparison with 
standard predictor like CART and Random Forest will better evaluate and judge the 
performance such new predictor. As for the tuner, there're many heuristic optimization 
algorithms in wild. However, DE is a good but maybe not the best candidate for the tuning 
process considering different performance measurements. To determine which optimizer is 
fitable and results in better performance is beyond this work scope. We leave it to future 
works. The rest of this section will describe each tool applied in this work.

\subsection{WHERE-based Learner}
\textbf{WHERE-based Learner} is composed of WHERE clustering algorithm and CART 
decision tree algorithm. The key idea of WHERE-based learner is that  instead of training the 
CART decision tree based on the class labels associated with each training sample, it's using 
CART to build decision trees based on the cluster labels, which are generated by the WHERE 
clustering algorithm.
 
 
% Given N instances, instead of building decision tree directly with CART, we use WHERE to 
%cluster instances. 
WHERE is a fast clustering algorithm designed by {\it menzies} for finding software artifacts 
with similar attributes. It clusters data on dimensions synthesized along the axis of greatest 
variability in the data. The way WHERE used to find such dimension is a linear-time heuristic 
called ``FASTMAP" proposed by Faloutsos \& Lin\cite{faloutsos1995fastmap}. ``FASTMAP" 
randomly picks one instance Z; find the instance {\it east} X that's furthest away form Z; find 
the instance {\it west} Y that's furthest away from {\it east} X. Next, project all the remaining 
points onto the line drawn between X and Y. Then choose the median point as the split and 
recursively divide all the instances into {\it west} and {\it east} clusters until the number of 
instances within each cluster is less than specific minimum size. The representation of the 
clustering result is a tree, we call it Where-clustering tree. Such tree will be pruned if applicable 
in some cases. Finally, cluster labels  will be assigned to instances in each of those 
clusters(leaves). 


Then based on the attributes and the cluster labels in each instances,  we build a 
decision tree based on CART algorithm. In each leaf of the tree, there're several instances 
falling in. This will be the model trained for defect prediction. During testing process, a new 
instance comes in and traverses the tree according to 
values of the its attributes. If the instance could reach one leaf of the tree, the predicted value 
of this instance will be the mean  of all the training data values associated with this leaf. 
Otherwise if stops at intermediate nodes of tree, the predicted value will be the mean of the all 
the training data value below this node. After estimating all the number of defectives in the test 
data, whether each instance contains defectives will be determined by comparing with a threshold value. If the predicted value is greater than threshold, the Where-based learner will predict this instance as defective otherwise non-defective.

%\begin{algorithm}
%	\caption{Pesudocode for WHERE Algorithm }
%	\label{alg:WHERE}
%		\begin{algorithmic}[1]
%			\Require $data$
%			\Ensure $tree$
%		\Function {Fastmap}{$ data$}
%			\State $one \gets$ any$(data)$
%			\State $west \gets$ furthest$(data, one)$
%			\State $east \gets$ furthest$(data, west)$
%			\State $c \gets$ distance($west, east$)
%		\EndFunction
%	 \end{algorithmic}            
%\end{algorithm}

\subsection{CART}
\textbf{CART} is an {\em iterative dichotomization} algorithm
that finds the attribute that most divides the data such that
the variance of the goal variable in each division is
minimized\cite{breiman84}.
The algorithm then recurses on each division.
Finally, the cost data in the leaf divisions
is averaged to generate the estimate.

\subsection{Random Forests}

Breiman's website describes \textbf{Random Forests} as
follows\cite{brieman00}. "Random Forests grows many classification
trees. To classify a new object from an input vector, put the input
vector down each of the trees in the forest. Each tree gives a
classification, and we say the tree "votes" for that class. The forest
chooses the classification having the most votes (over all the trees
in the forest). Each tree is grown as follows.
If the number of cases in the training set is N, sample N cases at
random - but with replacement, from the original data. This sample
will be the training set for growing the tree.
Also, if there are M input variables, a number m$<<$M is specified such
that at each node, m variables are selected at random out of the M and
the best split on these m is used to split the node. The value of m is
held constant during the forest growing.
Finally, each tree is grown to the largest extent possible (there is
no pruning)."




%Suppose the cordinates of X and Y are $(0,0)$ and $(0,c)$, which are east and west 
%instances within N samples. From the Pythegoras and cosine rule, each instance is projected 
%at the point(x,y)
%
%\begin{equation}
%	\begin{split}
%	x &= (a^2 + c^2 - b^2)/(2c) \\
%	y &= \sqrt{a^2 - x^2} 
%	\end{split}
%\end{equation}




\subsection{Differential Evolution Algorithm}
Differential Evolution (DE)\cite{storn1997differential} is a stochastic search algorithm that 
optimizes a problem by iteratively trying to improve a  population of candidate solutions with 
regard to a given quality measurement. Such method makes no assumptions about the 
problem being optimized and has already been used as a parameter tuner
\cite{omran2005differential, chiha2012tuning}.

DE starts with creating a population of candidate solutions and then generates new 
candidates based on $New = X+f*(Y-Z)$, where $X$, $Y$ and $Z$ are randomly selected 
solutions from the current frontier and $f$ is a crossover factor. The newly generated solution 
will be added into the next generation of solutions if dominating previous old points in the 
frontier. To avoid meaning less iteration, early termination strategy is applied that is at the 
beginning, assign a value to the $life$ parameter, it would be reduced by one each time   
when the new generation of candidate solutions does not improve in terms of quality 
measurement. The DE will stop when the $life$ equals to 0.

Algorithm \ref{alg:DE} is a list of pseudocode of DE with early termination for maximizing a 
score function, where $np$ is the number of population in each generation, $f$ is the 
crossover factor as mentioned, $cf$ is the probability for crossover operation to generate new 
candidate, $life$ is to control termination.

\begin{algorithm}
	\caption{Pesudocode for DE with Early Termination}
	\label{alg:DE}
		\begin{algorithmic}[1]
			\Require $np$, $f$, $cf$, $life$
			\Ensure $S_{best}$
		  \State $Population  \gets $ InitializePopulation($np$)
		  \State $S_{best} \gets $GetBestSolution($Population $)
		  \While{$life > 0$}
			\State $NewGeneration \gets \emptyset$
			\For{$i=0 \to n-1$}
				\State $S_i \gets$ GenNew($Population [i], Population , cf, f)$
				\If {Score($S_i$) >Score($Population [i]$)}
				 	\State $NewGeneration \gets S_i$
				\Else
					\State $NewGeneration \gets Population [i]$
				\EndIf
			\EndFor
			\State $Population  \gets NewGeneration$
			\If{$\neg$ Improve($Population $)}
				\State $life -=1$
			\EndIf
			\State $S_{best} \gets$ GetBestSolution($Population $)
		  \EndWhile
		 \State \Return $S_{best}$
                 \end{algorithmic}            
\end{algorithm}

%\begin{algorithm}
%\begin{algorithmic}[1]
% \KwData{this text}
% \KwResult{how to write algorithm with \LaTeX2e }
% initialization\;
% \While{not at end of this document}{
%  read current\;
%  \eIf{understand}{
%   go to next section\;
%   current section becomes this one\;
%   }{
%   go back to the beginning of current section\;
%  }
% }
% \caption{How to write algorithms}
% \end{algorithmic}
%\end{algorithm}

\section{Experiment}

\subsection{Data Set}

The data used in this study is from PROMISE repository. Ten software defect predicition data 
sets are analyzed. They're {\it ant}, {\it camel}, {\it ivy}, {\it jedit}, {\it log4j}, {\it lucene}, {\it 
synapse}, {\it velocity}, {\it xalan} and {\it xerces}. Each of these data sets is composed of 
several software modules with number of defects and code attributes. For more detailed 
description of code attributes and the original data sets, please refer to  http://openscience.us/
repo/

\subsection{Experiment Design}

The experiment aims at investigate whether tuning helps learners improve performance in 
terms of accuracy measurement. We choose compare WHERE-based learner with and 
without tuning parameters and two {\it significantly different} learners Random Forest and 
CART according to \cite{lessmann2008benchmarking}. As mentioned above, we'd like to see 
whether tuning will help change the rank of CART and make it comparable with Random 
Forest. 

To evaluate accuracy performance of learners, several measurements are proposed, like 
probability of  detection({\it pd}) and probability of false alarm ({\it pf})\cite{menzies2007data}, 
the area under the receiver operating characteristics curve ({\it AUC})
\cite{lessmann2008benchmarking}, and precision\cite{zhang2007comments}. In this work, we 
expect learners should identify as many defective modules as possible while avoiding false 
alarm. Therefore, learners are evaluated by both of {\it pd} and {\it pf} simaltanieously. A single 
measure, G-measure, defined as the harmonic mean of {\it pd} and $1-{\it pf}$  is used. The 
G-measure value is between 0 and 1. The higher, the better.
\begin{equation}
G = \frac{2*(1-pf)*pd}{1-pf+pd}
\end{equation}

In this experiment, we use three different portions of one project data set for training, tuning 
and testing process. In contrast to hold out way used in \cite{lessmann2008benchmarking, 
menzies2007data}, we separate the data sets in order. Since learners are designed to predict 
defects in future projects,  any randomly data set selection without taking into the time series 
will not sufficient to evaluate the performance of predicting future. To the most, that is good to 
evaluate the accuracy of classification but not predicting future. Since we have 10 different 
project data, each of which contains least 3 evolutionary versions. We use the following policy 
to select the data: in each project, we only use the last three data files for experiment. 
Specifically, the $nth$,  $(n-1)th$, $(n-2)th$ versions of project data are selected for testing, 
tuning and training learners, respectively. This will make sure that we don't use the future 
project data to train learners and predict previous project.

To investigates the impacts of parameters on learners, we use DE as the tuner and compare the G-measure 
values of Where-based learner with and without tuning, CART with and without tuning and Random 
Forests. Since the tuning time for Random Forests is very long, hopefully other researchers design
new heuristics to speed up the tuning process for Random Forests.  For the time being, even though
we don't tune Random Forests, if tuning help CART outperform Random Forests or improve itself performance ,
we still could conclude that tuning is helpful and necessary when comparing learners.

Besides the Where-based learner implemented by ourself,  we use the CART and Random Forest modules 
from scikit-learn \cite{scikit-learn} for this experiment. The parameters associated with different learners are listed in Fig.\ref{fig:parameters}. (\textbf{NEED to elaborate that there're different versions of CART, what's the point to do this experiment}). For each data set, run CART, naive Where-based learner and Random Forests with corresponding default values. Then using DE to tune corresponding parameters for CART and Where-based Learner,  and run them again with the optimal parameters from tuning process to test the performance. 
This study ranks learners using the Scott-Knott procedure recommended by Mittas \& Angelis in their 2013
IEEE TSE paper~\cite{mittas2013ranking}.  This method sorts a list of $l$ treatments with $ls$ measurements by their median score. It then splits $l$ into sub-lists $m,n$ in order to maximize the expected value of differences  in the observed performances before and after divisions. 






\begin{figure*}[t!]
\scriptsize
  \centering
	\begin{tabular}{|c|c|c|c|l|}
	\hline
	\begin{tabular}[c]{@{}c@{}}Learner \\ Name\end{tabular} & Parameters & Default &\begin{tabular}[c]{@{}c@{}}Tuning\\ Range\end{tabular}& 
\multicolumn{1}{c|}{Description} \\ \hline
	\multirow{8}{*}{\begin{tabular}[c]{@{}c@{}}Where-based\\ Learner\end{tabular}} 
	& threshold & 0.5 &[0,1]& The value to determine defective or not .\\ \cline{2-5} 
	& infogain & 0.33 &[0,1]& The percentage of features to consider  for the best 
split to build CART tree\footnote{Since the Where-based learner will build two trees, the first 
one is for clustering and the second one is building prediction model. we explicitly call Where-
clustering tree and CART tree, respectively}. \\ \cline{2-5} 
	 & min\_sample\_size & 4& [1,10]& The minimum number of samples required to be a leaf for 
CART tree. \\ \cline{2-5} 
	 & min\_Size & 0.5 &[0,1]& \begin{tabular}[c]{@{}l@{}}The value to determine the minimum 
number of samples to be a Where-clustering tree \\ based on  ${n\_samples}^ {min\_Size}$.
\end{tabular} \\ \cline{2-5} 
	 & depthMin & 2 & [1,6]&The minimum depth of the tree below which no pruning for Where-
clustering tree. \\ \cline{2-5} 
	 & depthMax & 10 &[1,20]& The maximum depth of the Where-clustering tree. \\ \cline{2-5} 
	 & wherePrune & False &T/F& Whether or not to prune the Where-clustering tree. \\ \cline{2-5} 
	 & treePrune & True &T/F& Whether or not to prune the classification tree built by CART. \\ 
\hline
	\multirow{5}{*}{CART} & threshold & 0.5 &[0,1]& The value to determine defective or not. \\ 
\cline{2-5} 
	 & max\_feature & None &[0.01,1]& The number of features to consider when looking for the best 
split. \\ \cline{2-5} 
	 & max\_depth & None &[1,50]& The maximum depth of the tree. \\ \cline{2-5} 
	 & min\_sample\_split & 2 &[2,20]& The minimum number of samples required to split an 
internal node. \\ \cline{2-5} 
	 & min\_smaples\_leaf & 1 & [1,20]&The minimum number of samples required to be at a leaf 
node. \\ \hline  
       \multirow{6}{*}{\begin{tabular}[c]{@{}c@{}}Random \\ Forests\end{tabular}}  & threshold & 0.5 & - & The value to determine defective or not. \\ 
\cline{2-5} 
	 & max\_feature & None &-& The number of features to consider when looking for the best 
split. \\ \cline{2-5} 
	 & max\_depth & None &-& The maximum depth of the tree. \\ \cline{2-5} 
	 & min\_sample\_split & 2 &-& The minimum number of samples required to split an 
internal node. \\ \cline{2-5} 
	 & min\_smaples\_leaf & 1 & -&The minimum number of samples required to be at a leaf 
node. \\ \cline{2-5} 
	 &  n\_estimators & 100 & -&The number of trees in the forest.\\ \hline

	\end{tabular}
    \caption {List of parameters to be tuned in Where-based learner and CART in scikit-learn.}
\label{fig:parameters}
\end{figure*}




\section{Result}
\begin{figure}[!th]

{\scriptsize
{\scriptsize \begin{tabular}{l@{~~~}l@{~~~}l@{~~~}r@{~~~}r@{~~~}c}
\arrayrulecolor{darkgray}
\rowcolor[gray]{0.75} Dataset     & Methods  & median &IQR&  \\ 
\rowcolor[gray]{.9}ant & Naive\_WHERE & 0 & 0 & \quart{0.0}{0}{0.0}\\
 & Naive\_Cart & 51 & 2 & \quart{51.0}{2}{52.0}\\
 & Naive\_RFst & 53 & 1 & \quart{52.0}{1}{53.0}\\
 & Tuned\_Cart & 63 & 0 & \quart{63.0}{0}{63.0}\\
 & Tuned\_WHERE & 72 & 0 & \quart{72.0}{0}{72.0}\\
\rowcolor[gray]{.9}camel & Naive\_WHERE & 45 & 0 & \quart{45.0}{0}{45.0}\\
 & Naive\_Cart & 54 & 2 & \quart{54.0}{2}{54.0}\\
 & Tuned\_WHERE & 56 & 0 & \quart{56.0}{0}{56.0}\\
 & Tuned\_Cart & 56 & 2 & \quart{56.0}{2}{57.0}\\
 & Naive\_RFst & 56 & 1 & \quart{57.0}{1}{57.0}\\
\rowcolor[gray]{.9}ivy & Naive\_RFst & 50 & 1 & \quart{50.0}{1}{50.0}\\
 & Naive\_WHERE & 54 & 0 & \quart{54.0}{0}{54.0}\\
 & Naive\_Cart & 57 & 1 & \quart{58.0}{1}{58.0}\\
 & Tuned\_Cart & 68 & 0 & \quart{68.0}{0}{68.0}\\
 & Tuned\_WHERE & 70 & 0 & \quart{70.0}{0}{70.0}\\
\rowcolor[gray]{.9} jedit & Naive\_WHERE & 49 & 0 & \quart{49.0}{0}{49.0}\\
 & Naive\_Cart & 55 & 15 & \quart{55.0}{15}{55.0}\\
 & Tuned\_WHERE & 62 & 0 & \quart{62.0}{0}{62.0}\\
 & Tuned\_Cart & 65 & 3 & \quart{65.0}{3}{66.0}\\
 & Naive\_RFst & 74 & 2 & \quart{73.0}{2}{74.0}\\
 \rowcolor[gray]{.9} log4j & Naive\_Cart & 48 & 1 & \quart{47.0}{1}{48.0}\\
 & Tuned\_WHERE & 50 & 0 & \quart{50.0}{0}{50.0}\\
 & Naive\_WHERE & 53 & 0 & \quart{53.0}{0}{53.0}\\
 & Tuned\_Cart & 55 & 4 & \quart{55.0}{4}{57.0}\\
 & Naive\_RFst & 56 & 0 & \quart{57.0}{0}{57.0}\\
\rowcolor[gray]{.9}lucene & Naive\_WHERE & 37 & 0 & \quart{37.0}{0}{37.0}\\
 & Naive\_RFst & 56 & 1 & \quart{56.0}{1}{56.0}\\
 & Naive\_Cart & 57 & 1 & \quart{57.0}{1}{58.0}\\
 & Tuned\_Cart & 57 & 4 & \quart{57.0}{4}{58.0}\\
 & Tuned\_WHERE & 61 & 0 & \quart{61.0}{0}{61.0}\\
\rowcolor[gray]{.9}synapse & Naive\_WHERE & 0 & 0 & \quart{0.0}{0}{0.0}\\
 & Naive\_Cart & 42 & 2 & \quart{41.0}{2}{42.0}\\
 & Naive\_RFst & 43 & 4 & \quart{43.0}{4}{44.0}\\
 & Tuned\_Cart & 49 & 11 & \quart{49.0}{11}{49.0}\\
 & Tuned\_WHERE & 64 & 0 & \quart{64.0}{0}{64.0}\\
\rowcolor[gray]{.9}velocity & Naive\_WHERE & 1 & 0 & \quart{1.0}{0}{1.0}\\
 & Naive\_RFst & 9 & 0 & \quart{9.0}{0}{9.0}\\
 & Naive\_Cart & 27 & 2 & \quart{26.0}{2}{27.0}\\
 & Tuned\_Cart & 49 & 5 & \quart{46.0}{5}{50.0}\\
 & Tuned\_WHERE & 57 & 0 & \quart{58.0}{0}{58.0}\\
\rowcolor[gray]{.9}xalan & Tuned\_Cart & 51 & 4 & \quart{49.0}{4}{51.0}\\
 & Tuned\_WHERE & 56 & 0 & \quart{57.0}{0}{57.0}\\
 & Naive\_Cart & 66 & 0 & \quart{66.0}{0}{66.0}\\
 & Naive\_WHERE & 69 & 0 & \quart{69.0}{0}{69.0}\\
 & Naive\_RFst & 73 & 1 & \quart{73.0}{1}{73.0}\\
\rowcolor[gray]{.9}xerces & Naive\_RFst & 18 & 1 & \quart{18.0}{1}{18.0}\\
 & Naive\_Cart & 19 & 1 & \quart{18.0}{1}{19.0}\\
 & Tuned\_Cart & 25 & 7 & \quart{24.0}{7}{26.0}\\
 & Naive\_WHERE & 26 & 0 & \quart{26.0}{0}{26.0}\\
 & Tuned\_WHERE & 34 & 1 & \quart{34.0}{1}{34.0}\\





 \end{tabular}}

}


\caption{Results over Different Data Sets}\label{fig:g feature}
\end{figure}

\section{Discussion}

\section{Related Work}

Tuning in efforts estimation, software engineering.

Defect Prediction



\section{Threats to Validity}

Internal and external threats

\section{Conclusion}

\section{Acknowledgments}

\bibliographystyle{unsrt}
\bibliography{tuningpredictor}  
\end{document}
